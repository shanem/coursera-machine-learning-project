---
title: "Weight Lifting Exercise"
author: "Shane Mooney"
date: "August 7, 2016"
output: html_document
---

# Setup
```{r}
library(caret)
set.seed(123)
```

```{r}
modelAccuracy <- function(model, testData) {
  prediction <- predict(model, testData)
  accuracy <- sum(prediction == testData$classe) / nrow(testData)
  accuracy
}
```


```{r}
pmlTraining <- read.csv("pml-training.csv")
pmlTesting <- read.csv("pml-testing.csv")
```

# Cleaning data

We exclude some data from our training and validation data sets:
 - rows with new_window == "yes" because the data in these rows looks different than other rows, and because no records in the test data set have "yes" for this column
 - columns either either NA or empty values. These columns are extremely sparesely populated, most having absolutely no actual values (after excluding new_window == "yes" rows), so we should be able to safely ignore them
 - We also disregard `X`, `new_window`, `num_window`, `cvtd_timestamp`, `raw_timestamp_part_1`, and `raw_timestamp_part_2` because the nature of these columns implies that they should have no predictive value

```{r}
goodPmlTraining <- pmlTraining[pmlTraining$new_window == "no", ]
goodPmlTraining <- goodPmlTraining[, colSums(is.na(goodPmlTraining)) == 0]
goodPmlTraining <- goodPmlTraining[, colSums(goodPmlTraining == "") == 0]

goodPmlTraining <- goodPmlTraining[ , !(names(goodPmlTraining) %in% c('X', 'new_window', 'num_window', 'cvtd_timestamp', 'raw_timestamp_part_1', 'raw_timestamp_part_2'))]
```

# Exploratory model training

Using a very small exploratory data set

```{r}
exploration <- goodPmlTraining[sample(nrow(goodPmlTraining), size=1000), ]
```

We train three models: A generalized linear model, a boosted regression model, and a random forest model.

```{r}
glmModel <- train(subset(exploration, TRUE, -c(classe)), exploration$classe, model="glm")
```

```{r}
gbmModel <- train(subset(exploration, TRUE, -c(classe)), exploration$classe, model="gbm")
```

```{r}
rfModel <- train(subset(exploration, TRUE, -c(classe)), exploration$classe, model="rf")
```

# Exploratory model accuracy

Validating these models against the entire training set:

```{r}
print(paste("GLM model accuracy: ", modelAccuracy(glmModel, goodPmlTraining)))
print(paste("GBM model accuracy: ", modelAccuracy(gbmModel, goodPmlTraining)))
print(paste("Random forest model accuracy: ", modelAccuracy(rfModel, goodPmlTraining)))
```

All three perform roughly equally well. We arbitrarily choose random forest, and train our final model on a full sized training set.

# Variable importance

Examining the top 20 most important predictors, as reported by the exploratory random forest:

```{r}
varImp(rfModel)
```

`roll_belt` and `yaw_belt` appear to be the two strongest predictor, so we generate a scatterplot to explore the relationship

```{r}
qplot(roll_belt, yaw_belt, colour = classe, data = exploration)
```

We see a few very clear trends with these two predictors alone

# Training final model

Because of strength of exploratory models and slowness of building them, we'll choose a relatively small subset of the training data to train our final model on, reserving the remainder for validation

```{r}
trainingSampleSize <- floor(0.25 * nrow(goodPmlTraining))
trainingIndices <- sample(seq_len(nrow(goodPmlTraining)), size = trainingSampleSize)

training <- goodPmlTraining[trainingIndices, ]
validation <- goodPmlTraining[-trainingIndices, ]
```


```{r}
finalModel <- train(subset(training, TRUE, -c(classe)), training$classe, model="rf")
```

# Validation

We evaluate our model against the validation data set

```{r}
print(paste("Accuracy on validation data set: ", modelAccuracy(finalModel, validation)))
```

We expect the out of sample error rate to be roughly 1%

# Testing
```{r}
testPredictions <- predict(rfModel, testing[, colnames(rfModel$trainingData)[0:(length(rfModel$trainingData) - 1)]])
print("Test predictions:")
data.frame(testPredictions)
```


# Citations

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

Read more: http://groupware.les.inf.puc-rio.br/har#ixzz4GhRgF9oF

# Licenses

Important: you are free to use this dataset for any purpose. This dataset is licensed under the Creative Commons license (CC BY-SA). The CC BY-SA license means you can remix, tweak, and build upon this work even for commercial purposes, as long as you credit the authors of the original work and you license your new creations under the identical terms we are licensing to you. This license is often compared to "copyleft" free and open source software licenses. All new works based on this dataset will carry the same license, so any derivatives will also allow commercial use.